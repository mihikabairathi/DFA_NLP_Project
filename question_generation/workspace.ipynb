{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     /home/elias/nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/elias/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/elias/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('state_union')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "import re\n",
    "from nltk.parse.stanford import StanfordDependencyParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elias/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "dependancy_parser = StanfordDependencyParser(\n",
    "    path_to_jar = 'stanford-parser-full-2018-10-17/stanford-parser.jar',\n",
    "    path_to_models_jar = 'stanford-english-corenlp-2018-02-27-models.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list_iterator' object has no attribute 'nodes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-1916157faea7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tag'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'VB'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list_iterator' object has no attribute 'nodes'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elias/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "dependancy_parser = StanfordDependencyParser(\n",
    "    path_to_jar = 'stanford-parser-full-2018-10-17/stanford-parser.jar',\n",
    "    path_to_models_jar = 'stanford-english-corenlp-2018-02-27-models.jar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_for_dep_with_pos(tree, deps, collection, pos_list):\n",
    "    for pos in pos_list:\n",
    "        if pos in deps.keys():\n",
    "            for j in deps[pos]:\n",
    "                collection.append((tree.nodes[j]['word'], pos))\n",
    "    return collection\n",
    "\n",
    "def clean_search_results(search_results):\n",
    "    new_results = []\n",
    "    for guess in search_results:\n",
    "        valid = True\n",
    "        for pos in guess.keys():\n",
    "            if guess[pos] == []:\n",
    "                valid = False\n",
    "        if valid:\n",
    "            new_results.append(guess)\n",
    "    return new_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function DependencyGraph.__init__.<locals>.<lambda> at 0x7efcd3168d90>,\n",
      "            {0: {'address': 0,\n",
      "                 'ctag': 'TOP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'root': [4]}),\n",
      "                 'feats': None,\n",
      "                 'head': None,\n",
      "                 'lemma': None,\n",
      "                 'rel': None,\n",
      "                 'tag': 'TOP',\n",
      "                 'word': None},\n",
      "             1: {'address': 1,\n",
      "                 'ctag': 'NNP',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'nsubj',\n",
      "                 'tag': 'NNP',\n",
      "                 'word': 'Pittsburgh'},\n",
      "             2: {'address': 2,\n",
      "                 'ctag': 'VBZ',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'cop',\n",
      "                 'tag': 'VBZ',\n",
      "                 'word': 'is'},\n",
      "             3: {'address': 3,\n",
      "                 'ctag': 'IN',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'case',\n",
      "                 'tag': 'IN',\n",
      "                 'word': 'in'},\n",
      "             4: {'address': 4,\n",
      "                 'ctag': 'NNP',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'case': [3],\n",
      "                                      'cop': [2],\n",
      "                                      'nsubj': [1]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 0,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'root',\n",
      "                 'tag': 'NNP',\n",
      "                 'word': 'Pennsylvania'}})\n"
     ]
    }
   ],
   "source": [
    "s = 'Pittsburgh is in Pennsylvania'\n",
    "result = dependancy_parser.raw_parse(s)\n",
    "for node in result: print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Subject': [('Pittsburgh', 'nsubj')], 'Descriptor': ['Pennsylvania'], 'Verb': [('is', 'cop')], 'Case': [('', 'ph'), ('in', 'case')]}]\n"
     ]
    }
   ],
   "source": [
    "def get_verb_subj_dobj(tree):\n",
    "    results = []\n",
    "    for i in range(len(tree.nodes)):\n",
    "        if tree.nodes[i]['tag'][:2] == 'VB':\n",
    "            subject = []\n",
    "            dir_object = []\n",
    "            verb = []\n",
    "            verb.append(tree.nodes[i]['word'])\n",
    "            deps = tree.nodes[i]['deps']\n",
    "            subject = search_for_dep_with_pos(tree, deps, subject, ['nsubj', 'nsubjpass'])\n",
    "            dir_object = search_for_dep_with_pos(tree, deps, dir_object, ['nmod', 'dobj'])\n",
    "            verb = search_for_dep_with_pos(tree, deps, verb, ['auxpass'])\n",
    "            results.append({\"Subject\": subject, \"Direct Object\": dir_object, \"Verb\": verb})\n",
    "    return clean_search_results(results)\n",
    "\n",
    "def get_noun_descriptor(tree):\n",
    "    results = []\n",
    "    for i in range(len(tree.nodes)):\n",
    "        subject = []\n",
    "        descriptor = []\n",
    "        verb = []\n",
    "        case = [('', 'ph')]\n",
    "        deps = tree.nodes[i]['deps']\n",
    "        descriptor.append((tree.nodes[i]['word']))\n",
    "        subject = search_for_dep_with_pos(tree, deps, subject, ['nsubj'])\n",
    "        verb = search_for_dep_with_pos(tree, deps, verb, ['cop'])\n",
    "        case = search_for_dep_with_pos(tree, deps, case, ['case'])\n",
    "        results.append({\"Subject\": subject, \"Descriptor\": descriptor, \"Verb\": verb, \"Case\": case})\n",
    "    return clean_search_results(results)\n",
    "                   \n",
    "s = 'Pittsburgh is in Pennsylvania'\n",
    "result = dependancy_parser.raw_parse(s)\n",
    "for node in result: print(get_noun_descriptor(node))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q_templates = [\n",
    "\"When did {} {}?\",\n",
    "\"Where did {} {}?\",\n",
    "\"What did {} do?\",\n",
    "\"What did {} do in {}?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_templates = [ \n",
    "    \"When did {} {}?\", \n",
    "    \"Where did {} {}?\", \n",
    "    \"What did {} do?\", \n",
    "    \"What did {} do in {}?\", \n",
    "    \"Who {}?\"\n",
    "]\n",
    "\n",
    "class question_template:\n",
    "    \n",
    "    def __init__(self, string):\n",
    "        self.string = string\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(sentence):\n",
    "    parse_tree = dependancy_parser.raw_parse(sentence)\n",
    "    dep = next(parse_tree)\n",
    "    simplified = list(dep.triples())\n",
    "    get_subjects(simplified, sentence)\n",
    "    get_objects(simplified, sentence)\n",
    "    print(simplified)\n",
    "    \n",
    "def get_subjects(pt, sentence):\n",
    "    all_subjects = []\n",
    "    for entry in pt:\n",
    "        if entry[1] == 'nsubj' or entry[1] == 'nmod':\n",
    "            subject = [entry[2][0]]\n",
    "            for entry_pass_2 in pt:\n",
    "                if entry_pass_2[0][0] == entry[2][0] and entry_pass_2[1] == 'compound':\n",
    "                    subject.append(entry_pass_2[2][0])\n",
    "            return_subject = ''\n",
    "\n",
    "            for word in re.sub(',','',sentence).split():\n",
    "                if word in subject:\n",
    "                    return_subject += word\n",
    "                    return_subject += ' '\n",
    "            if entry[0][1][:2] == 'VB':\n",
    "                print('What did {} {}?'.format(return_subject, entry[0][0]))\n",
    "            all_subjects.append(return_subject)\n",
    "    return all_subjects\n",
    "\n",
    "def get_objects(pt, sentence):\n",
    "    all_objects = []\n",
    "    for entry in pt:\n",
    "        if entry[1] == 'dobj' or entry[1] == 'nsubjpass':\n",
    "            object_ = [entry[2][0]]\n",
    "            for entry_pass_2 in pt:\n",
    "                if entry_pass_2[0][0] == entry[2][0] and entry_pass_2[1] == 'compound':\n",
    "                    object_.append(entry_pass_2[2][0])\n",
    "            return_object = ''\n",
    "\n",
    "            for word in re.sub(',','',sentence).split():\n",
    "                if word in object_:\n",
    "                    return_object += word\n",
    "                    return_object += ' '\n",
    "            if entry[0][1][:2] == 'VB':\n",
    "                print('Who {} {}?'.format(entry[0][0], return_object))\n",
    "            all_objects.append(return_object)\n",
    "    return all_objects\n",
    "\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
