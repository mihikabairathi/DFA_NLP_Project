{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package state_union to\n",
      "[nltk_data]     /home/elias/nltk_data...\n",
      "[nltk_data]   Package state_union is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/elias/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/elias/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('state_union')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "import re\n",
    "from nltk.parse.stanford import StanfordDependencyParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elias/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "dependancy_parser = StanfordDependencyParser(\n",
    "    path_to_jar = 'stanford-parser-full-2018-10-17/stanford-parser.jar',\n",
    "    path_to_models_jar = 'stanford-english-corenlp-2018-02-27-models.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'Joe went to the grocery store in downtown Pittsburgh'\n",
    "result = dependancy_parser.raw_parse(s)\n",
    "for node in result:\n",
    "    result = node\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'address': 0, 'word': None, 'lemma': None, 'ctag': 'TOP', 'tag': 'TOP', 'feats': None, 'head': None, 'deps': defaultdict(<class 'list'>, {'root': [2]}), 'rel': None}\n",
      "{'address': 1, 'word': 'Joe', 'lemma': '_', 'ctag': 'NNP', 'tag': 'NNP', 'feats': '_', 'head': 2, 'deps': defaultdict(<class 'list'>, {}), 'rel': 'nsubj'}\n",
      "{'address': 2, 'word': 'went', 'lemma': '_', 'ctag': 'VBD', 'tag': 'VBD', 'feats': '_', 'head': 0, 'deps': defaultdict(<class 'list'>, {'nsubj': [1], 'nmod': [6, 9]}), 'rel': 'root'}\n",
      "{'address': 3, 'word': 'to', 'lemma': '_', 'ctag': 'TO', 'tag': 'TO', 'feats': '_', 'head': 6, 'deps': defaultdict(<class 'list'>, {}), 'rel': 'case'}\n",
      "{'address': 4, 'word': 'the', 'lemma': '_', 'ctag': 'DT', 'tag': 'DT', 'feats': '_', 'head': 6, 'deps': defaultdict(<class 'list'>, {}), 'rel': 'det'}\n",
      "{'address': 5, 'word': 'grocery', 'lemma': '_', 'ctag': 'NN', 'tag': 'NN', 'feats': '_', 'head': 6, 'deps': defaultdict(<class 'list'>, {}), 'rel': 'compound'}\n",
      "{'address': 6, 'word': 'store', 'lemma': '_', 'ctag': 'NN', 'tag': 'NN', 'feats': '_', 'head': 2, 'deps': defaultdict(<class 'list'>, {'case': [3], 'det': [4], 'compound': [5]}), 'rel': 'nmod'}\n",
      "{'address': 7, 'word': 'in', 'lemma': '_', 'ctag': 'IN', 'tag': 'IN', 'feats': '_', 'head': 9, 'deps': defaultdict(<class 'list'>, {}), 'rel': 'case'}\n",
      "{'address': 8, 'word': 'downtown', 'lemma': '_', 'ctag': 'JJ', 'tag': 'JJ', 'feats': '_', 'head': 9, 'deps': defaultdict(<class 'list'>, {}), 'rel': 'amod'}\n",
      "{'address': 9, 'word': 'Pittsburgh', 'lemma': '_', 'ctag': 'NNP', 'tag': 'NNP', 'feats': '_', 'head': 2, 'deps': defaultdict(<class 'list'>, {'case': [7], 'amod': [8]}), 'rel': 'nmod'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(result.nodes)):\n",
    "    print(result.nodes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'address': 2, 'word': 'went', 'lemma': '_', 'ctag': 'VBD', 'tag': 'VBD', 'feats': '_', 'head': 0, 'deps': defaultdict(<class 'list'>, {'nsubj': [1], 'nmod': [6, 9]}), 'rel': 'root'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(result.nodes)):\n",
    "    if result.nodes[i]['tag'][:2] == 'VB':\n",
    "        print(result.nodes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elias/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: The StanfordDependencyParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPDependencyParser\u001b[0m instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "dependancy_parser = StanfordDependencyParser(\n",
    "    path_to_jar = 'stanford-parser-full-2018-10-17/stanford-parser.jar',\n",
    "    path_to_models_jar = 'stanford-english-corenlp-2018-02-27-models.jar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function DependencyGraph.__init__.<locals>.<lambda> at 0x7efcf01dad90>,\n",
      "            {0: {'address': 0,\n",
      "                 'ctag': 'TOP',\n",
      "                 'deps': defaultdict(<class 'list'>, {'root': [4]}),\n",
      "                 'feats': None,\n",
      "                 'head': None,\n",
      "                 'lemma': None,\n",
      "                 'rel': None,\n",
      "                 'tag': 'TOP',\n",
      "                 'word': None},\n",
      "             1: {'address': 1,\n",
      "                 'ctag': 'NNP',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'nsubj',\n",
      "                 'tag': 'NNP',\n",
      "                 'word': 'Joe'},\n",
      "             2: {'address': 2,\n",
      "                 'ctag': 'VBZ',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'cop',\n",
      "                 'tag': 'VBZ',\n",
      "                 'word': 'is'},\n",
      "             3: {'address': 3,\n",
      "                 'ctag': 'DT',\n",
      "                 'deps': defaultdict(<class 'list'>, {}),\n",
      "                 'feats': '_',\n",
      "                 'head': 4,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'det',\n",
      "                 'tag': 'DT',\n",
      "                 'word': 'an'},\n",
      "             4: {'address': 4,\n",
      "                 'ctag': 'NN',\n",
      "                 'deps': defaultdict(<class 'list'>,\n",
      "                                     {'cop': [2],\n",
      "                                      'det': [3],\n",
      "                                      'nsubj': [1]}),\n",
      "                 'feats': '_',\n",
      "                 'head': 0,\n",
      "                 'lemma': '_',\n",
      "                 'rel': 'root',\n",
      "                 'tag': 'NN',\n",
      "                 'word': 'engineer'}})\n"
     ]
    }
   ],
   "source": [
    "s = 'Joe is an engineer'\n",
    "result = dependancy_parser.raw_parse(s)\n",
    "for node in result: print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Subject': [], 'Descriptor': [None], 'Verb': []}, {'Subject': [], 'Descriptor': ['Joe'], 'Verb': []}, {'Subject': [], 'Descriptor': ['was'], 'Verb': []}, {'Subject': [], 'Descriptor': ['an'], 'Verb': []}, {'Subject': [('Joe', 'nsubj')], 'Descriptor': ['engineer'], 'Verb': [('was', 'cop')]}]\n"
     ]
    }
   ],
   "source": [
    "def get_verb_subj_dobj(tree):\n",
    "    results = []\n",
    "    for i in range(len(tree.nodes)):\n",
    "        if tree.nodes[i]['tag'][:2] == 'VB':\n",
    "            subject = []\n",
    "            dir_object = []\n",
    "            verb = []\n",
    "            verb.append(tree.nodes[i]['word'])\n",
    "            deps = tree.nodes[i]['deps']\n",
    "            for pos in ['nsubj', 'nsubjpass']:\n",
    "                if pos in deps.keys():\n",
    "                    for j in deps[pos]:\n",
    "                        subject.append((tree.nodes[j]['word'], pos))\n",
    "            for pos in ['nmod', 'dobj']:\n",
    "                if pos in deps.keys():\n",
    "                    for j in deps[pos]:\n",
    "                        dir_object.append((tree.nodes[j]['word'], pos))\n",
    "            for pos in ['auxpass']:\n",
    "                if pos in deps.keys():\n",
    "                    for j in deps[pos]:\n",
    "                        verb.append((tree.nodes[j]['word'], pos))\n",
    "            results.append({\"Subject\": subject, \"Direct Object\": dir_object, \"Verb\": verb})\n",
    "    return results\n",
    "\n",
    "def get_noun_descriptor(tree):\n",
    "    results = []\n",
    "    for i in range(len(tree.nodes)):\n",
    "        subject = []\n",
    "        descriptor = []\n",
    "        verb = []\n",
    "        deps = tree.nodes[i]['deps']\n",
    "        descriptor.append(tree.nodes[i]['word'])\n",
    "        for pos in ['nsubj']:\n",
    "            if pos in deps.keys():\n",
    "                for j in deps[pos]:\n",
    "                    subject.append((tree.nodes[j]['word'], pos))\n",
    "        for pos in ['cop']:\n",
    "            if pos in deps.keys():\n",
    "                for j in deps[pos]:\n",
    "                    verb.append((tree.nodes[j]['word'], pos))\n",
    "        results.append({\"Subject\": subject, \"Descriptor\": descriptor, \"Verb\": verb})\n",
    "    return results\n",
    "                \n",
    "                \n",
    "            \n",
    "    \n",
    "        \n",
    "    \n",
    "s = 'Joe was an engineer'\n",
    "result = dependancy_parser.raw_parse(s)\n",
    "for node in result: print(get_noun_descriptor(node))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q_templates = [\n",
    "\"When did {} {}?\",\n",
    "\"Where did {} {}?\",\n",
    "\"What did {} do?\",\n",
    "\"What did {} do in {}?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_templates = [ \n",
    "    \"When did {} {}?\", \n",
    "    \"Where did {} {}?\", \n",
    "    \"What did {} do?\", \n",
    "    \"What did {} do in {}?\", \n",
    "    \"Who {}?\"\n",
    "]\n",
    "\n",
    "class question_template:\n",
    "    \n",
    "    def __init__(self, string):\n",
    "        self.string = string\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(sentence):\n",
    "    parse_tree = dependancy_parser.raw_parse(sentence)\n",
    "    dep = next(parse_tree)\n",
    "    simplified = list(dep.triples())\n",
    "    get_subjects(simplified, sentence)\n",
    "    get_objects(simplified, sentence)\n",
    "    print(simplified)\n",
    "    \n",
    "def get_subjects(pt, sentence):\n",
    "    all_subjects = []\n",
    "    for entry in pt:\n",
    "        if entry[1] == 'nsubj' or entry[1] == 'nmod':\n",
    "            subject = [entry[2][0]]\n",
    "            for entry_pass_2 in pt:\n",
    "                if entry_pass_2[0][0] == entry[2][0] and entry_pass_2[1] == 'compound':\n",
    "                    subject.append(entry_pass_2[2][0])\n",
    "            return_subject = ''\n",
    "\n",
    "            for word in re.sub(',','',sentence).split():\n",
    "                if word in subject:\n",
    "                    return_subject += word\n",
    "                    return_subject += ' '\n",
    "            if entry[0][1][:2] == 'VB':\n",
    "                print('What did {} {}?'.format(return_subject, entry[0][0]))\n",
    "            all_subjects.append(return_subject)\n",
    "    return all_subjects\n",
    "\n",
    "def get_objects(pt, sentence):\n",
    "    all_objects = []\n",
    "    for entry in pt:\n",
    "        if entry[1] == 'dobj' or entry[1] == 'nsubjpass':\n",
    "            object_ = [entry[2][0]]\n",
    "            for entry_pass_2 in pt:\n",
    "                if entry_pass_2[0][0] == entry[2][0] and entry_pass_2[1] == 'compound':\n",
    "                    object_.append(entry_pass_2[2][0])\n",
    "            return_object = ''\n",
    "\n",
    "            for word in re.sub(',','',sentence).split():\n",
    "                if word in object_:\n",
    "                    return_object += word\n",
    "                    return_object += ' '\n",
    "            if entry[0][1][:2] == 'VB':\n",
    "                print('Who {} {}?'.format(entry[0][0], return_object))\n",
    "            all_objects.append(return_object)\n",
    "    return all_objects\n",
    "\n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dependancy_parser' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1f6fdb691969>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_questions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'John drove his car, and Jane rode her bike'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-d2d15e358ccf>\u001b[0m in \u001b[0;36mgenerate_questions\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_questions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mparse_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdependancy_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msimplified\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtriples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mget_subjects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimplified\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dependancy_parser' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'as'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, True, True]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FizzBuzz\n",
      "1\n",
      "2\n",
      "Fizz\n",
      "4\n",
      "Buzz\n",
      "Fizz\n",
      "7\n",
      "8\n",
      "Fizz\n",
      "Buzz\n",
      "11\n",
      "Fizz\n",
      "13\n",
      "14\n",
      "FizzBuzz\n",
      "16\n",
      "17\n",
      "Fizz\n",
      "19\n",
      "Buzz\n",
      "Fizz\n",
      "22\n",
      "23\n",
      "Fizz\n",
      "Buzz\n",
      "26\n",
      "Fizz\n",
      "28\n",
      "29\n",
      "FizzBuzz\n",
      "31\n",
      "32\n",
      "Fizz\n",
      "34\n",
      "Buzz\n",
      "Fizz\n",
      "37\n",
      "38\n",
      "Fizz\n",
      "Buzz\n",
      "41\n",
      "Fizz\n",
      "43\n",
      "44\n",
      "FizzBuzz\n",
      "46\n",
      "47\n",
      "Fizz\n",
      "49\n",
      "Buzz\n",
      "Fizz\n",
      "52\n",
      "53\n",
      "Fizz\n",
      "Buzz\n",
      "56\n",
      "Fizz\n",
      "58\n",
      "59\n",
      "FizzBuzz\n",
      "61\n",
      "62\n",
      "Fizz\n",
      "64\n",
      "Buzz\n",
      "Fizz\n",
      "67\n",
      "68\n",
      "Fizz\n",
      "Buzz\n",
      "71\n",
      "Fizz\n",
      "73\n",
      "74\n",
      "FizzBuzz\n",
      "76\n",
      "77\n",
      "Fizz\n",
      "79\n",
      "Buzz\n",
      "Fizz\n",
      "82\n",
      "83\n",
      "Fizz\n",
      "Buzz\n",
      "86\n",
      "Fizz\n",
      "88\n",
      "89\n",
      "FizzBuzz\n",
      "91\n",
      "92\n",
      "Fizz\n",
      "94\n",
      "Buzz\n",
      "Fizz\n",
      "97\n",
      "98\n",
      "Fizz\n",
      "Buzz\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(3 and 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
